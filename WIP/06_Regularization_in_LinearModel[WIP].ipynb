{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3653090",
   "metadata": {},
   "source": [
    "**What is the benefit of having alternative fitting procedures in linear models?** \n",
    "<br>\n",
    "* Improve accuracy\n",
    "* Improve interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57cc36",
   "metadata": {},
   "source": [
    "If the relationship between X(input) and y(output) is linear, then the model have low bias! But, how about variance? Does our model have low variance as well? **yes if `n >> p`.** Otherwise, i.e n is not much larger than p, then we can have a lot of variability. (why? think about data with two features, and think how many poitns do we need to plot a line in 2D space? this should give you some idea!). IF `p > n`, then there is no longer a unique least square coefficient estimate. But, if we find a a way to shrink some of those features, then our model will can less variability.  So, we increase bias a little bit to gain low variance.\n",
    "<br>\n",
    "Furthermore, setting coef of some variables to 0, we can get into a model that is better in terms  of interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f5df91",
   "metadata": {},
   "source": [
    "**Three common methods to be used with linear model:** \n",
    "<br>\n",
    "* subset selection (select a subset of features rather than using all features)\n",
    "* shrinkage(regularization): that makes some of coefs (of variables) close to zeros. (in a particular method, we can make some of those coef zero so to perform feature selection!)\n",
    "* Dim reduction: we project p-dim data into m-dim space, where `m < p`. Then, we use the new variables (in m-dim space) and fit a linear model on them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b125a952",
   "metadata": {},
   "source": [
    "# Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d996ee30",
   "metadata": {},
   "source": [
    "## Best subset selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7f8d35",
   "metadata": {},
   "source": [
    "This is a brute-force approach that search all possible options and choose the best one! So, for a data with `p` features, the algorithm is as follows:\n",
    "\n",
    "* for each k in {1,..,p} \n",
    "* fit each of C(p, k) models and peak the best one (e.g. the one with largest  $R^{2}$). We call this: `Model-k`\n",
    "* Choose the best Model out of {Model-1, ..., Model-p} using cross-validation.\n",
    "\n",
    "Note that to choose the best model out of set {Model-1, ..., Model-p}, we cannot use $R^{2}$ as this score is usually higher for models with higher number of predictors. So, we can use adjusted $R^{2}$. Other options are $C_{p} (AIC)$ and BIC. **(more on this later!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1204f69e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
